{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 2: Overview of Automated Evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load API tokens for our 3rd party APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "from utils import get_circle_api_key\n",
    "cci_api_key = get_circle_api_key()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "from utils import get_gh_api_key\n",
    "gh_api_key = get_gh_api_key()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "from utils import get_openai_api_key\n",
    "openai_api_key = get_openai_api_key()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up our github branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "height": 64
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CircleCI-Learning/llmops-course'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import get_repo_name\n",
    "course_repo = get_repo_name()\n",
    "course_repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "height": 64
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dl-cci-skilled-sale-49'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import get_branch\n",
    "course_branch = get_branch()\n",
    "course_branch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The sample application: AI-powered quiz generator\n",
    "We are going to build a AI powered quiz generator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the dataset for the quizz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "human_template  = \"{question}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "height": 608
   },
   "outputs": [],
   "source": [
    "quiz_bank = \"\"\"1. Subject: Leonardo DaVinci\n",
    "   Categories: Art, Science\n",
    "   Facts:\n",
    "    - Painted the Mona Lisa\n",
    "    - Studied zoology, anatomy, geology, optics\n",
    "    - Designed a flying machine\n",
    "  \n",
    "2. Subject: Paris\n",
    "   Categories: Art, Geography\n",
    "   Facts:\n",
    "    - Location of the Louvre, the museum where the Mona Lisa is displayed\n",
    "    - Capital of France\n",
    "    - Most populous city in France\n",
    "    - Where Radium and Polonium were discovered by scientists Marie and Pierre Curie\n",
    "\n",
    "3. Subject: Telescopes\n",
    "   Category: Science\n",
    "   Facts:\n",
    "    - Device to observe different objects\n",
    "    - The first refracting telescopes were invented in the Netherlands in the 17th Century\n",
    "    - The James Webb space telescope is the largest telescope in space. It uses a gold-berillyum mirror\n",
    "\n",
    "4. Subject: Starry Night\n",
    "   Category: Art\n",
    "   Facts:\n",
    "    - Painted by Vincent van Gogh in 1889\n",
    "    - Captures the east-facing view of van Gogh's room in Saint-Rémy-de-Provence\n",
    "\n",
    "5. Subject: Physics\n",
    "   Category: Science\n",
    "   Facts:\n",
    "    - The sun doesn't change color during sunset.\n",
    "    - Water slows the speed of light\n",
    "    - The Eiffel Tower in Paris is taller in the summer than the winter due to expansion of the metal.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the prompt template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "height": 540
   },
   "outputs": [],
   "source": [
    "delimiter = \"####\"\n",
    "\n",
    "prompt_template = f\"\"\"\n",
    "Follow these steps to generate a customized quiz for the user.\n",
    "The question will be delimited with four hashtags i.e {delimiter}\n",
    "\n",
    "The user will provide a category that they want to create a quiz for. Any questions included in the quiz\n",
    "should only refer to the category.\n",
    "\n",
    "Step 1:{delimiter} First identify the category user is asking about from the following list:\n",
    "* Geography\n",
    "* Science\n",
    "* Art\n",
    "\n",
    "Step 2:{delimiter} Determine the subjects to generate questions about. The list of topics are below:\n",
    "\n",
    "{quiz_bank}\n",
    "\n",
    "Pick up to two subjects that fit the user's category. \n",
    "\n",
    "Step 3:{delimiter} Generate a quiz for the user. Based on the selected subjects generate 3 questions for the user using the facts about the subject.\n",
    "\n",
    "Use the following format for the quiz:\n",
    "Question 1:{delimiter} <question 1>\n",
    "\n",
    "Question 2:{delimiter} <question 2>\n",
    "\n",
    "Question 3:{delimiter} <question 3>\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use langchain to build the prompt template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "chat_prompt = ChatPromptTemplate.from_messages([(\"human\", prompt_template)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "height": 47
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=[], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template=\"\\nFollow these steps to generate a customized quiz for the user.\\nThe question will be delimited with four hashtags i.e ####\\n\\nThe user will provide a category that they want to create a quiz for. Any questions included in the quiz\\nshould only refer to the category.\\n\\nStep 1:#### First identify the category user is asking about from the following list:\\n* Geography\\n* Science\\n* Art\\n\\nStep 2:#### Determine the subjects to generate questions about. The list of topics are below:\\n\\n1. Subject: Leonardo DaVinci\\n   Categories: Art, Science\\n   Facts:\\n    - Painted the Mona Lisa\\n    - Studied zoology, anatomy, geology, optics\\n    - Designed a flying machine\\n  \\n2. Subject: Paris\\n   Categories: Art, Geography\\n   Facts:\\n    - Location of the Louvre, the museum where the Mona Lisa is displayed\\n    - Capital of France\\n    - Most populous city in France\\n    - Where Radium and Polonium were discovered by scientists Marie and Pierre Curie\\n\\n3. Subject: Telescopes\\n   Category: Science\\n   Facts:\\n    - Device to observe different objects\\n    - The first refracting telescopes were invented in the Netherlands in the 17th Century\\n    - The James Webb space telescope is the largest telescope in space. It uses a gold-berillyum mirror\\n\\n4. Subject: Starry Night\\n   Category: Art\\n   Facts:\\n    - Painted by Vincent van Gogh in 1889\\n    - Captures the east-facing view of van Gogh's room in Saint-Rémy-de-Provence\\n\\n5. Subject: Physics\\n   Category: Science\\n   Facts:\\n    - The sun doesn't change color during sunset.\\n    - Water slows the speed of light\\n    - The Eiffel Tower in Paris is taller in the summer than the winter due to expansion of the metal.\\n\\nPick up to two subjects that fit the user's category. \\n\\nStep 3:#### Generate a quiz for the user. Based on the selected subjects generate 3 questions for the user using the facts about the subject.\\n\\nUse the following format for the quiz:\\nQuestion 1:#### <question 1>\\n\\nQuestion 2:#### <question 2>\\n\\nQuestion 3:#### <question 3>\\n\\n\"))])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print to observe the content or generated object\n",
    "chat_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "height": 64
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, temperature=0.0, openai_api_key='eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJhcHAiLCJleHAiOjE3OTk5OTk5OTksInN1YiI6NjgwMjA0LCJhdWQiOiJXRUIiLCJpYXQiOjE2OTQwNzY4NTF9.lp_i6geUnSR3sVwp1bsyrU7QRoXnzTVP9Heg1EViM08', openai_api_base='http://jupyter-api-proxy.internal.dlai/rev-proxy', openai_organization='', openai_proxy='')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up an output parser in LangChain that converts the llm response into a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "height": 81
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StrOutputParser()"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parser\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "output_parser = StrOutputParser()\n",
    "output_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect the pieces using the pipe operator from Langchain Expression Language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "height": 47
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=[], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template=\"\\nFollow these steps to generate a customized quiz for the user.\\nThe question will be delimited with four hashtags i.e ####\\n\\nThe user will provide a category that they want to create a quiz for. Any questions included in the quiz\\nshould only refer to the category.\\n\\nStep 1:#### First identify the category user is asking about from the following list:\\n* Geography\\n* Science\\n* Art\\n\\nStep 2:#### Determine the subjects to generate questions about. The list of topics are below:\\n\\n1. Subject: Leonardo DaVinci\\n   Categories: Art, Science\\n   Facts:\\n    - Painted the Mona Lisa\\n    - Studied zoology, anatomy, geology, optics\\n    - Designed a flying machine\\n  \\n2. Subject: Paris\\n   Categories: Art, Geography\\n   Facts:\\n    - Location of the Louvre, the museum where the Mona Lisa is displayed\\n    - Capital of France\\n    - Most populous city in France\\n    - Where Radium and Polonium were discovered by scientists Marie and Pierre Curie\\n\\n3. Subject: Telescopes\\n   Category: Science\\n   Facts:\\n    - Device to observe different objects\\n    - The first refracting telescopes were invented in the Netherlands in the 17th Century\\n    - The James Webb space telescope is the largest telescope in space. It uses a gold-berillyum mirror\\n\\n4. Subject: Starry Night\\n   Category: Art\\n   Facts:\\n    - Painted by Vincent van Gogh in 1889\\n    - Captures the east-facing view of van Gogh's room in Saint-Rémy-de-Provence\\n\\n5. Subject: Physics\\n   Category: Science\\n   Facts:\\n    - The sun doesn't change color during sunset.\\n    - Water slows the speed of light\\n    - The Eiffel Tower in Paris is taller in the summer than the winter due to expansion of the metal.\\n\\nPick up to two subjects that fit the user's category. \\n\\nStep 3:#### Generate a quiz for the user. Based on the selected subjects generate 3 questions for the user using the facts about the subject.\\n\\nUse the following format for the quiz:\\nQuestion 1:#### <question 1>\\n\\nQuestion 2:#### <question 2>\\n\\nQuestion 3:#### <question 3>\\n\\n\"))])\n",
       "| ChatOpenAI(client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, temperature=0.0, openai_api_key='eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJhcHAiLCJleHAiOjE3OTk5OTk5OTksInN1YiI6NjgwMjA0LCJhdWQiOiJXRUIiLCJpYXQiOjE2OTQwNzY4NTF9.lp_i6geUnSR3sVwp1bsyrU7QRoXnzTVP9Heg1EViM08', openai_api_base='http://jupyter-api-proxy.internal.dlai/rev-proxy', openai_organization='', openai_proxy='')\n",
       "| StrOutputParser()"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = chat_prompt | llm | output_parser\n",
    "chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the function 'assistance_chain' to put together all steps above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "height": 217
   },
   "outputs": [],
   "source": [
    "# taking all components and making reusable as one piece\n",
    "def assistant_chain(\n",
    "    system_message,\n",
    "    human_template=\"{question}\",\n",
    "    llm=ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0),\n",
    "    output_parser=StrOutputParser()):\n",
    "  \n",
    "  chat_prompt = ChatPromptTemplate.from_messages([\n",
    "      (\"system\", system_message),\n",
    "      (\"human\", human_template),\n",
    "  ])\n",
    "  return chat_prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the function 'eval_expected_words' for the first example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "height": 404
   },
   "outputs": [],
   "source": [
    "def eval_expected_words(\n",
    "    system_message,\n",
    "    question,\n",
    "    expected_words,\n",
    "    human_template=\"{question}\",\n",
    "    llm=ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0),\n",
    "    output_parser=StrOutputParser()):\n",
    "    \n",
    "  assistant = assistant_chain(\n",
    "      system_message,\n",
    "      human_template,\n",
    "      llm,\n",
    "      output_parser)\n",
    "    \n",
    "  \n",
    "  answer = assistant.invoke({\"question\": question})\n",
    "    \n",
    "  print(answer)\n",
    "    \n",
    "  assert any(word in answer.lower() \\\n",
    "             for word in expected_words), \\\n",
    "    f\"Expected the assistant questions to include \\\n",
    "    '{expected_words}', but it did not\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test: Generate a quiz about science."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "question  = \"Generate a quiz about science.\"\n",
    "expected_words = [\"davinci\", \"telescope\", \"physics\", \"curie\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the eval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "height": 98
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1:#### First identify the category user is asking about from the following list:\n",
      "* Geography\n",
      "* Science\n",
      "* Art\n",
      "\n",
      "User selected category: Science\n",
      "\n",
      "Step 2:#### Determine the subjects to generate questions about. The list of topics are below:\n",
      "\n",
      "1. Subject: Telescopes\n",
      "   Category: Science\n",
      "   Facts:\n",
      "    - Device to observe different objects\n",
      "    - The first refracting telescopes were invented in the Netherlands in the 17th Century\n",
      "    - The James Webb space telescope is the largest telescope in space. It uses a gold-berillyum mirror\n",
      "\n",
      "2. Subject: Physics\n",
      "   Category: Science\n",
      "   Facts:\n",
      "    - The sun doesn't change color during sunset.\n",
      "    - Water slows the speed of light\n",
      "    - The Eiffel Tower in Paris is taller in the summer than the winter due to expansion of the metal.\n",
      "\n",
      "Selected subjects: Telescopes, Physics\n",
      "\n",
      "Step 3:#### Generate a quiz for the user. Based on the selected subjects generate 3 questions for the user using the facts about the subject.\n",
      "\n",
      "Question 1:#### What is the James Webb space telescope known for?\n",
      "a) Inventing the first refracting telescope\n",
      "b) Using a gold-berillyum mirror\n",
      "c) Discovering the expansion of the Eiffel Tower\n",
      "d) Observing the sun's color change during sunset\n",
      "\n",
      "Question 2:#### How does water affect the speed of light according to physics?\n",
      "a) Speeds up the light\n",
      "b) Slows down the light\n",
      "c) Changes the color of the light\n",
      "d) Stops the light completely\n",
      "\n",
      "Question 3:#### Why is the Eiffel Tower taller in the summer than in the winter?\n",
      "a) Due to the effect of gravity\n",
      "b) Expansion of the metal in the summer\n",
      "c) It is an optical illusion\n",
      "d) The tower is actually the same height throughout the year\n"
     ]
    }
   ],
   "source": [
    "eval_expected_words(\n",
    "    prompt_template,\n",
    "    question,\n",
    "    expected_words\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the function 'evaluate_refusal' to define a failing test case where the app should decline to answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "height": 336
   },
   "outputs": [],
   "source": [
    "def evaluate_refusal(\n",
    "    system_message,\n",
    "    question,\n",
    "    decline_response,\n",
    "    human_template=\"{question}\", \n",
    "    llm=ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0),\n",
    "    output_parser=StrOutputParser()):\n",
    "    \n",
    "  assistant = assistant_chain(human_template, \n",
    "                              system_message,\n",
    "                              llm,\n",
    "                              output_parser)\n",
    "  \n",
    "  answer = assistant.invoke({\"question\": question})\n",
    "  print(answer)\n",
    "  \n",
    "  assert decline_response.lower() in answer.lower(), \\\n",
    "    f\"Expected the bot to decline with \\\n",
    "    '{decline_response}' got {answer}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a new question (which should be a bad request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "question  = \"Generate a quiz about Rome.\"\n",
    "decline_response = \"I'm sorry\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the refusal eval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background-color:pink; padding:15px;\"> <b>Note:</b> The following function call will throw an exception.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "height": 98
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### First identify the category user is asking about from the following list:\n",
      "* Geography\n",
      "* Science\n",
      "* Art\n",
      "\n",
      "User's Category: Geography\n",
      "\n",
      "#### Determine the subjects to generate questions about:\n",
      "\n",
      "1. Subject: Paris\n",
      "   Categories: Art, Geography\n",
      "   Facts:\n",
      "    - Location of the Louvre, the museum where the Mona Lisa is displayed\n",
      "    - Capital of France\n",
      "    - Most populous city in France\n",
      "    - Where Radium and Polonium were discovered by scientists Marie and Pierre Curie\n",
      "\n",
      "2. Subject: Telescopes\n",
      "   Category: Science\n",
      "   Facts:\n",
      "    - Device to observe different objects\n",
      "    - The first refracting telescopes were invented in the Netherlands in the 17th Century\n",
      "    - The James Webb space telescope is the largest telescope in space. It uses a gold-berillyum mirror\n",
      "\n",
      "#### Generate a quiz for the user:\n",
      "\n",
      "Question 1: What is the capital of France?\n",
      "a) Rome\n",
      "b) Paris\n",
      "c) London\n",
      "d) Berlin\n",
      "\n",
      "Question 2: Where were Radium and Polonium discovered by Marie and Pierre Curie?\n",
      "a) London\n",
      "b) Paris\n",
      "c) Berlin\n",
      "d) Rome\n",
      "\n",
      "Question 3: What is the James Webb space telescope known for?\n",
      "a) Using a silver mirror\n",
      "b) Being the smallest telescope in space\n",
      "c) Invented in the 19th Century\n",
      "d) Using a gold-berillyum mirror\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Expected the bot to decline with     'I'm sorry' got #### First identify the category user is asking about from the following list:\n* Geography\n* Science\n* Art\n\nUser's Category: Geography\n\n#### Determine the subjects to generate questions about:\n\n1. Subject: Paris\n   Categories: Art, Geography\n   Facts:\n    - Location of the Louvre, the museum where the Mona Lisa is displayed\n    - Capital of France\n    - Most populous city in France\n    - Where Radium and Polonium were discovered by scientists Marie and Pierre Curie\n\n2. Subject: Telescopes\n   Category: Science\n   Facts:\n    - Device to observe different objects\n    - The first refracting telescopes were invented in the Netherlands in the 17th Century\n    - The James Webb space telescope is the largest telescope in space. It uses a gold-berillyum mirror\n\n#### Generate a quiz for the user:\n\nQuestion 1: What is the capital of France?\na) Rome\nb) Paris\nc) London\nd) Berlin\n\nQuestion 2: Where were Radium and Polonium discovered by Marie and Pierre Curie?\na) London\nb) Paris\nc) Berlin\nd) Rome\n\nQuestion 3: What is the James Webb space telescope known for?\na) Using a silver mirror\nb) Being the smallest telescope in space\nc) Invented in the 19th Century\nd) Using a gold-berillyum mirror",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mevaluate_refusal\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt_template\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecline_response\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[19], line 17\u001b[0m, in \u001b[0;36mevaluate_refusal\u001b[0;34m(system_message, question, decline_response, human_template, llm, output_parser)\u001b[0m\n\u001b[1;32m     14\u001b[0m answer \u001b[38;5;241m=\u001b[39m assistant\u001b[38;5;241m.\u001b[39minvoke({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m: question})\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(answer)\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m decline_response\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01min\u001b[39;00m answer\u001b[38;5;241m.\u001b[39mlower(), \\\n\u001b[1;32m     18\u001b[0m   \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected the bot to decline with \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124m  \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdecline_response\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00manswer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Expected the bot to decline with     'I'm sorry' got #### First identify the category user is asking about from the following list:\n* Geography\n* Science\n* Art\n\nUser's Category: Geography\n\n#### Determine the subjects to generate questions about:\n\n1. Subject: Paris\n   Categories: Art, Geography\n   Facts:\n    - Location of the Louvre, the museum where the Mona Lisa is displayed\n    - Capital of France\n    - Most populous city in France\n    - Where Radium and Polonium were discovered by scientists Marie and Pierre Curie\n\n2. Subject: Telescopes\n   Category: Science\n   Facts:\n    - Device to observe different objects\n    - The first refracting telescopes were invented in the Netherlands in the 17th Century\n    - The James Webb space telescope is the largest telescope in space. It uses a gold-berillyum mirror\n\n#### Generate a quiz for the user:\n\nQuestion 1: What is the capital of France?\na) Rome\nb) Paris\nc) London\nd) Berlin\n\nQuestion 2: Where were Radium and Polonium discovered by Marie and Pierre Curie?\na) London\nb) Paris\nc) Berlin\nd) Rome\n\nQuestion 3: What is the James Webb space telescope known for?\na) Using a silver mirror\nb) Being the smallest telescope in space\nc) Invented in the 19th Century\nd) Using a gold-berillyum mirror"
     ]
    }
   ],
   "source": [
    "evaluate_refusal(\n",
    "    prompt_template,\n",
    "    question,\n",
    "    decline_response\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running evaluations in a CircleCI pipeline\n",
    "\n",
    "Put all these steps together into files to reuse later.\n",
    "\n",
    "**_Note:_** fixing the system_message by adding additional rules:\n",
    "\n",
    "- Only use explicit matches for the category, if the category is not an exact match to categories in the quiz bank, answer that you do not have information.\n",
    "- If the user asks a question about a subject you do not have information about in the quiz bank, answer \"I'm sorry I do not have information about that\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "height": 1594
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app.py\n",
    "from langchain.prompts                import ChatPromptTemplate\n",
    "from langchain.chat_models            import ChatOpenAI\n",
    "from langchain.schema.output_parser   import StrOutputParser\n",
    "\n",
    "delimiter = \"####\"\n",
    "\n",
    "quiz_bank = \"\"\"1. Subject: Leonardo DaVinci\n",
    "   Categories: Art, Science\n",
    "   Facts:\n",
    "    - Painted the Mona Lisa\n",
    "    - Studied zoology, anatomy, geology, optics\n",
    "    - Designed a flying machine\n",
    "  \n",
    "2. Subject: Paris\n",
    "   Categories: Art, Geography\n",
    "   Facts:\n",
    "    - Location of the Louvre, the museum where the Mona Lisa is displayed\n",
    "    - Capital of France\n",
    "    - Most populous city in France\n",
    "    - Where Radium and Polonium were discovered by scientists Marie and Pierre Curie\n",
    "\n",
    "3. Subject: Telescopes\n",
    "   Category: Science\n",
    "   Facts:\n",
    "    - Device to observe different objects\n",
    "    - The first refracting telescopes were invented in the Netherlands in the 17th Century\n",
    "    - The James Webb space telescope is the largest telescope in space. It uses a gold-berillyum mirror\n",
    "\n",
    "4. Subject: Starry Night\n",
    "   Category: Art\n",
    "   Facts:\n",
    "    - Painted by Vincent van Gogh in 1889\n",
    "    - Captures the east-facing view of van Gogh's room in Saint-Rémy-de-Provence\n",
    "\n",
    "5. Subject: Physics\n",
    "   Category: Science\n",
    "   Facts:\n",
    "    - The sun doesn't change color during sunset.\n",
    "    - Water slows the speed of light\n",
    "    - The Eiffel Tower in Paris is taller in the summer than the winter due to expansion of the metal.\n",
    "\"\"\"\n",
    "\n",
    "system_message = f\"\"\"\n",
    "Follow these steps to generate a customized quiz for the user.\n",
    "The question will be delimited with four hashtags i.e {delimiter}\n",
    "\n",
    "The user will provide a category that they want to create a quiz for. Any questions included in the quiz\n",
    "should only refer to the category.\n",
    "\n",
    "Step 1:{delimiter} First identify the category user is asking about from the following list:\n",
    "* Geography\n",
    "* Science\n",
    "* Art\n",
    "\n",
    "Step 2:{delimiter} Determine the subjects to generate questions about. The list of topics are below:\n",
    "\n",
    "{quiz_bank}\n",
    "\n",
    "Pick up to two subjects that fit the user's category. \n",
    "\n",
    "Step 3:{delimiter} Generate a quiz for the user. Based on the selected subjects generate 3 questions for the user using the facts about the subject.\n",
    "\n",
    "Use the following format for the quiz:\n",
    "Question 1:{delimiter} <question 1>\n",
    "\n",
    "Question 2:{delimiter} <question 2>\n",
    "\n",
    "Question 3:{delimiter} <question 3>\n",
    "\n",
    "Additional rules:\n",
    "\n",
    "- Only use explicit matches for the category, if the category is not an exact match to categories in the quiz bank, answer that you do not have information.\n",
    "- If the user asks a question about a subject you do not have information about in the quiz bank, answer \"I'm sorry I do not have information about that\".\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "  Helper functions for writing the test cases\n",
    "\"\"\"\n",
    "\n",
    "def assistant_chain(\n",
    "    system_message=system_message,\n",
    "    human_template=\"{question}\",\n",
    "    llm=ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0),\n",
    "    output_parser=StrOutputParser()):\n",
    "\n",
    "  chat_prompt = ChatPromptTemplate.from_messages([\n",
    "      (\"system\", system_message),\n",
    "      (\"human\", human_template),\n",
    "  ])\n",
    "  return chat_prompt | llm | output_parser\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Command to see the content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from langchain.prompts                import ChatPromptTemplate\r\n",
      "from langchain.chat_models            import ChatOpenAI\r\n",
      "from langchain.schema.output_parser   import StrOutputParser\r\n",
      "\r\n",
      "delimiter = \"####\"\r\n",
      "\r\n",
      "quiz_bank = \"\"\"1. Subject: Leonardo DaVinci\r\n",
      "   Categories: Art, Science\r\n",
      "   Facts:\r\n",
      "    - Painted the Mona Lisa\r\n",
      "    - Studied zoology, anatomy, geology, optics\r\n",
      "    - Designed a flying machine\r\n",
      "  \r\n",
      "2. Subject: Paris\r\n",
      "   Categories: Art, Geography\r\n",
      "   Facts:\r\n",
      "    - Location of the Louvre, the museum where the Mona Lisa is displayed\r\n",
      "    - Capital of France\r\n",
      "    - Most populous city in France\r\n",
      "    - Where Radium and Polonium were discovered by scientists Marie and Pierre Curie\r\n",
      "\r\n",
      "3. Subject: Telescopes\r\n",
      "   Category: Science\r\n",
      "   Facts:\r\n",
      "    - Device to observe different objects\r\n",
      "    - The first refracting telescopes were invented in the Netherlands in the 17th Century\r\n",
      "    - The James Webb space telescope is the largest telescope in space. It uses a gold-berillyum mirror\r\n",
      "\r\n",
      "4. Subject: Starry Night\r\n",
      "   Category: Art\r\n",
      "   Facts:\r\n",
      "    - Painted by Vincent van Gogh in 1889\r\n",
      "    - Captures the east-facing view of van Gogh's room in Saint-Rémy-de-Provence\r\n",
      "\r\n",
      "5. Subject: Physics\r\n",
      "   Category: Science\r\n",
      "   Facts:\r\n",
      "    - The sun doesn't change color during sunset.\r\n",
      "    - Water slows the speed of light\r\n",
      "    - The Eiffel Tower in Paris is taller in the summer than the winter due to expansion of the metal.\r\n",
      "\"\"\"\r\n",
      "\r\n",
      "system_message = f\"\"\"\r\n",
      "Follow these steps to generate a customized quiz for the user.\r\n",
      "The question will be delimited with four hashtags i.e {delimiter}\r\n",
      "\r\n",
      "The user will provide a category that they want to create a quiz for. Any questions included in the quiz\r\n",
      "should only refer to the category.\r\n",
      "\r\n",
      "Step 1:{delimiter} First identify the category user is asking about from the following list:\r\n",
      "* Geography\r\n",
      "* Science\r\n",
      "* Art\r\n",
      "\r\n",
      "Step 2:{delimiter} Determine the subjects to generate questions about. The list of topics are below:\r\n",
      "\r\n",
      "{quiz_bank}\r\n",
      "\r\n",
      "Pick up to two subjects that fit the user's category. \r\n",
      "\r\n",
      "Step 3:{delimiter} Generate a quiz for the user. Based on the selected subjects generate 3 questions for the user using the facts about the subject.\r\n",
      "\r\n",
      "Use the following format for the quiz:\r\n",
      "Question 1:{delimiter} <question 1>\r\n",
      "\r\n",
      "Question 2:{delimiter} <question 2>\r\n",
      "\r\n",
      "Question 3:{delimiter} <question 3>\r\n",
      "\r\n",
      "Additional rules:\r\n",
      "\r\n",
      "- Only use explicit matches for the category, if the category is not an exact match to categories in the quiz bank, answer that you do not have information.\r\n",
      "- If the user asks a question about a subject you do not have information about in the quiz bank, answer \"I'm sorry I do not have information about that\".\r\n",
      "\"\"\"\r\n",
      "\r\n",
      "\"\"\"\r\n",
      "  Helper functions for writing the test cases\r\n",
      "\"\"\"\r\n",
      "\r\n",
      "def assistant_chain(\r\n",
      "    system_message=system_message,\r\n",
      "    human_template=\"{question}\",\r\n",
      "    llm=ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0),\r\n",
      "    output_parser=StrOutputParser()):\r\n",
      "\r\n",
      "  chat_prompt = ChatPromptTemplate.from_messages([\r\n",
      "      (\"system\", system_message),\r\n",
      "      (\"human\", human_template),\r\n",
      "  ])\r\n",
      "  return chat_prompt | llm | output_parser\r\n"
     ]
    }
   ],
   "source": [
    "!cat app.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create new file to include the evals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "height": 1322
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_assistant.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_assistant.py\n",
    "from app import assistant_chain\n",
    "from app import system_message\n",
    "from langchain.prompts                import ChatPromptTemplate\n",
    "from langchain.chat_models            import ChatOpenAI\n",
    "from langchain.schema.output_parser   import StrOutputParser\n",
    "\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "def eval_expected_words(\n",
    "    system_message,\n",
    "    question,\n",
    "    expected_words,\n",
    "    human_template=\"{question}\",\n",
    "    llm=ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0),\n",
    "    output_parser=StrOutputParser()):\n",
    "\n",
    "  assistant = assistant_chain(system_message)\n",
    "  answer = assistant.invoke({\"question\": question})\n",
    "  print(answer)\n",
    "    \n",
    "  assert any(word in answer.lower() \\\n",
    "             for word in expected_words), \\\n",
    "    f\"Expected the assistant questions to include \\\n",
    "    '{expected_words}', but it did not\"\n",
    "\n",
    "def evaluate_refusal(\n",
    "    system_message,\n",
    "    question,\n",
    "    decline_response,\n",
    "    human_template=\"{question}\", \n",
    "    llm=ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0),\n",
    "    output_parser=StrOutputParser()):\n",
    "    \n",
    "  assistant = assistant_chain(human_template, \n",
    "                              system_message,\n",
    "                              llm,\n",
    "                              output_parser)\n",
    "  \n",
    "  answer = assistant.invoke({\"question\": question})\n",
    "  print(answer)\n",
    "  \n",
    "  assert decline_response.lower() in answer.lower(), \\\n",
    "    f\"Expected the bot to decline with \\\n",
    "    '{decline_response}' got {answer}\"\n",
    "\n",
    "\"\"\"\n",
    "  Test cases\n",
    "\"\"\"\n",
    "\n",
    "def test_science_quiz():\n",
    "  \n",
    "  question  = \"Generate a quiz about science.\"\n",
    "  expected_subjects = [\"davinci\", \"telescope\", \"physics\", \"curie\"]\n",
    "  eval_expected_words(\n",
    "      system_message,\n",
    "      question,\n",
    "      expected_subjects)\n",
    "\n",
    "def test_geography_quiz():\n",
    "  question  = \"Generate a quiz about geography.\"\n",
    "  expected_subjects = [\"paris\", \"france\", \"louvre\"]\n",
    "  eval_expected_words(\n",
    "      system_message,\n",
    "      question,\n",
    "      expected_subjects)\n",
    "\n",
    "def test_refusal_rome():\n",
    "  question  = \"Help me create a quiz about Rome\"\n",
    "  decline_response = \"I'm sorry\"\n",
    "  evaluate_refusal(\n",
    "      system_message,\n",
    "      question,\n",
    "      decline_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Command to see the content of the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from app import assistant_chain\r\n",
      "from app import system_message\r\n",
      "from langchain.prompts                import ChatPromptTemplate\r\n",
      "from langchain.chat_models            import ChatOpenAI\r\n",
      "from langchain.schema.output_parser   import StrOutputParser\r\n",
      "\r\n",
      "import os\r\n",
      "\r\n",
      "from dotenv import load_dotenv, find_dotenv\r\n",
      "_ = load_dotenv(find_dotenv())\r\n",
      "\r\n",
      "def eval_expected_words(\r\n",
      "    system_message,\r\n",
      "    question,\r\n",
      "    expected_words,\r\n",
      "    human_template=\"{question}\",\r\n",
      "    llm=ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0),\r\n",
      "    output_parser=StrOutputParser()):\r\n",
      "\r\n",
      "  assistant = assistant_chain(system_message)\r\n",
      "  answer = assistant.invoke({\"question\": question})\r\n",
      "  print(answer)\r\n",
      "    \r\n",
      "  assert any(word in answer.lower() \\\r\n",
      "             for word in expected_words), \\\r\n",
      "    f\"Expected the assistant questions to include \\\r\n",
      "    '{expected_words}', but it did not\"\r\n",
      "\r\n",
      "def evaluate_refusal(\r\n",
      "    system_message,\r\n",
      "    question,\r\n",
      "    decline_response,\r\n",
      "    human_template=\"{question}\", \r\n",
      "    llm=ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0),\r\n",
      "    output_parser=StrOutputParser()):\r\n",
      "    \r\n",
      "  assistant = assistant_chain(human_template, \r\n",
      "                              system_message,\r\n",
      "                              llm,\r\n",
      "                              output_parser)\r\n",
      "  \r\n",
      "  answer = assistant.invoke({\"question\": question})\r\n",
      "  print(answer)\r\n",
      "  \r\n",
      "  assert decline_response.lower() in answer.lower(), \\\r\n",
      "    f\"Expected the bot to decline with \\\r\n",
      "    '{decline_response}' got {answer}\"\r\n",
      "\r\n",
      "\"\"\"\r\n",
      "  Test cases\r\n",
      "\"\"\"\r\n",
      "\r\n",
      "def test_science_quiz():\r\n",
      "  \r\n",
      "  question  = \"Generate a quiz about science.\"\r\n",
      "  expected_subjects = [\"davinci\", \"telescope\", \"physics\", \"curie\"]\r\n",
      "  eval_expected_words(\r\n",
      "      system_message,\r\n",
      "      question,\r\n",
      "      expected_subjects)\r\n",
      "\r\n",
      "def test_geography_quiz():\r\n",
      "  question  = \"Generate a quiz about geography.\"\r\n",
      "  expected_subjects = [\"paris\", \"france\", \"louvre\"]\r\n",
      "  eval_expected_words(\r\n",
      "      system_message,\r\n",
      "      question,\r\n",
      "      expected_subjects)\r\n",
      "\r\n",
      "def test_refusal_rome():\r\n",
      "  question  = \"Help me create a quiz about Rome\"\r\n",
      "  decline_response = \"I'm sorry\"\r\n",
      "  evaluate_refusal(\r\n",
      "      system_message,\r\n",
      "      question,\r\n",
      "      decline_response)\r\n"
     ]
    }
   ],
   "source": [
    "!cat test_assistant.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The CircleCI config file\n",
    "Now let's set up our tests to run automatically in CircleCI.\n",
    "\n",
    "For this course, we've created a working CircleCI config file. Let's take a look at the configuration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "version: 2.1\r\n",
      "orbs:\r\n",
      "  # The python orb contains a set of prepackaged circleci configuration you can use repeatedly in your configurations files\r\n",
      "  # Orb commands and jobs help you with common scripting around a language/tool\r\n",
      "  # so you dont have to copy and paste it everywhere.\r\n",
      "  # See the orb documentation here: https://circleci.com/developer/orbs/orb/circleci/python\r\n",
      "  python: circleci/python@2.1.1\r\n",
      "\r\n",
      "parameters:\r\n",
      "  eval-mode:\r\n",
      "    type: string\r\n",
      "    default: \"commit\"\r\n",
      "\r\n",
      "workflows:\r\n",
      "  evaluate-commit:\r\n",
      "    when:\r\n",
      "      equal: [ commit, << pipeline.parameters.eval-mode >> ]\r\n",
      "    jobs:\r\n",
      "      - run-commit-evals:\r\n",
      "          context:\r\n",
      "            - dl-ai-courses\r\n",
      "  evaluate-release:\r\n",
      "    when:\r\n",
      "      equal: [ release, << pipeline.parameters.eval-mode >> ]\r\n",
      "    jobs:\r\n",
      "      - run-pre-release-evals:\r\n",
      "          context:\r\n",
      "            - dl-ai-courses\r\n",
      "  evaluate-all:\r\n",
      "    when:\r\n",
      "      equal: [ full, << pipeline.parameters.eval-mode >> ]\r\n",
      "    jobs:\r\n",
      "      - run-manual-evals:\r\n",
      "          context:\r\n",
      "            - dl-ai-courses\r\n",
      "  report-evals:\r\n",
      "    when:\r\n",
      "      equal: [ report, << pipeline.parameters.eval-mode >> ]\r\n",
      "    jobs:\r\n",
      "      - store-eval-artifacts:\r\n",
      "          context:\r\n",
      "            - dl-ai-courses\r\n",
      "\r\n",
      "jobs:\r\n",
      "  run-commit-evals:  # This is the name of the job, feel free to change it to better match what you're trying to do!\r\n",
      "    # These next lines defines a docker executors: https://circleci.com/docs/2.0/executor-types/\r\n",
      "    # You can specify an image from dockerhub or use one of the convenience images from CircleCI's Developer Hub\r\n",
      "    # A list of available CircleCI docker convenience images are available here: https://circleci.com/developer/images/image/cimg/python\r\n",
      "    # The executor is the environment in which the steps below will be executed - below will use a python 3.9 container\r\n",
      "    # Change the version below to your required version of python\r\n",
      "    docker:\r\n",
      "      - image: cimg/python:3.10.5\r\n",
      "    # Checkout the code as the first step. This is a dedicated CircleCI step.\r\n",
      "    # The python orb's install-packages step will install the dependencies from a Pipfile via Pipenv by default.\r\n",
      "    # Here we're making sure we use just use the system-wide pip. By default it uses the project root's requirements.txt.\r\n",
      "    # Then run your tests!\r\n",
      "    # CircleCI will report the results back to your VCS provider.\r\n",
      "    steps:\r\n",
      "      - checkout\r\n",
      "      - python/install-packages:\r\n",
      "          pkg-manager: pip\r\n",
      "          # app-dir: ~/project/package-directory/  # If your requirements.txt isn't in the root directory.\r\n",
      "          # pip-dependency-file: test-requirements.txt  # if you have a different name for your requirements file, maybe one that combines your runtime and test requirements.\r\n",
      "      - run:\r\n",
      "          name: Run assistant evals.\r\n",
      "          command: python -m pytest --junitxml results.xml test_assistant.py\r\n",
      "      - store_test_results:\r\n",
      "          path: results.xml\r\n",
      "  run-pre-release-evals:\r\n",
      "    docker:\r\n",
      "      - image: cimg/python:3.10.5\r\n",
      "    steps:\r\n",
      "      - checkout\r\n",
      "      - python/install-packages:\r\n",
      "          pkg-manager: pip\r\n",
      "          # app-dir: ~/project/package-directory/  # If your requirements.txt isn't in the root directory.\r\n",
      "          # pip-dependency-file: test-requirements.txt  # if you have a different name for your requirements file, maybe one that combines your runtime and test requirements.\r\n",
      "      - run:\r\n",
      "          name: Run release evals.\r\n",
      "          command: python -m pytest --junitxml results.xml test_release_evals.py\r\n",
      "      - store_test_results:\r\n",
      "          path: results.xml\r\n",
      "  run-manual-evals: \r\n",
      "    docker:\r\n",
      "      - image: cimg/python:3.10.5\r\n",
      "    steps:\r\n",
      "      - checkout\r\n",
      "      - python/install-packages:\r\n",
      "          pkg-manager: pip\r\n",
      "          # app-dir: ~/project/package-directory/  # If your requirements.txt isn't in the root directory.\r\n",
      "          # pip-dependency-file: test-requirements.txt  # if you have a different name for your requirements file, maybe one that combines your runtime and test requirements.\r\n",
      "      - run:\r\n",
      "          name: Run end to end evals.\r\n",
      "          command: python -m pytest --junitxml results.xml test_assistant.py test_release_evals.py\r\n",
      "      - store_test_results:\r\n",
      "          path: results.xml\r\n",
      "  store-eval-artifacts:\r\n",
      "    docker:\r\n",
      "      - image: cimg/python:3.10.5\r\n",
      "    steps:\r\n",
      "      - checkout\r\n",
      "      - python/install-packages:\r\n",
      "          pkg-manager: pip\r\n",
      "          # app-dir: ~/project/package-directory/  # If your requirements.txt isn't in the root directory.\r\n",
      "          # pip-dependency-file: test-requirements.txt  # if you have a different name for your requirements file, maybe one that combines your runtime and test requirements.\r\n",
      "      - run:\r\n",
      "          name: Save eval to html file\r\n",
      "          command: python save_eval_artifacts.py\r\n",
      "      - store_artifacts:\r\n",
      "          path: /tmp/eval_results.html\r\n",
      "          destination: eval_results.html"
     ]
    }
   ],
   "source": [
    "!cat circle_config.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the per-commit evals\n",
    "Push files into the github repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "height": 64
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uploading test_assistant.py\n",
      "uploading app.py\n",
      "pushing files to: dl-cci-skilled-sale-49\n"
     ]
    }
   ],
   "source": [
    "from utils import push_files\n",
    "push_files(course_repo, course_branch, [\"app.py\", \"test_assistant.py\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trigger the pipeline in CircleCI pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "height": 47
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please visit https://app.circleci.com/pipelines/github/CircleCI-Learning/llmops-course/6210\n"
     ]
    }
   ],
   "source": [
    "from utils import trigger_commit_evals\n",
    "trigger_commit_evals(course_repo, course_branch, cci_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
